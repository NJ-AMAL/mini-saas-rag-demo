Mini SaaS â€“ DÃ©mo RAG Multi-Client
=================================

Ceci est une mini-application SaaS simulant un systÃ¨me RAG multi-client.  
Chaque client possÃ¨de son propre espace de documents et ne peut interroger que ses propres documents.  
Les rÃ©ponses sont strictement limitÃ©es aux documents du client.

------------------------------------------------------------
1. Backend
------------------------------------------------------------

PrÃ©requis :
- Python 3.12+
- Installer les dÃ©pendances :

    pip install -r requirements.txt

DÃ©marrer le backend :

    uvicorn app:app --reload

Le backend sera accessible sur http://127.0.0.1:8000

------------------------------------------------------------
2. Frontend (Streamlit)
------------------------------------------------------------

DÃ©marrer lâ€™interface Streamlit :

    streamlit run ui.py

Le frontend sâ€™ouvrira dans votre navigateur.  
Vous pourrez entrer votre clÃ© API et poser des questions sur vos documents.

------------------------------------------------------------
3. ClÃ©s API (pour tests)
------------------------------------------------------------

Client   | ClÃ© API
---------|---------
clientA  | tenantA_key
clientB  | tenantB_key

> Utilisez le champ "API Key" dans lâ€™interface Streamlit. Cela simule lâ€™en-tÃªte `X-API-KEY`.

------------------------------------------------------------
4. Tests par client
------------------------------------------------------------

1. Saisir la clÃ© API du client.
2. Poser une question.
3. Le systÃ¨me renverra des rÃ©ponses **uniquement Ã  partir des documents de ce client**.
4. Essayer une clÃ© client sur les documents dâ€™un autre client renverra une erreur 404.

------------------------------------------------------------
5. Notes
------------------------------------------------------------

- Le backend utilise **FAISS + SentenceTransformer** pour intÃ©grer et rechercher dans les documents.
- Les rÃ©ponses sont directement extraites des documents â€” aucun LLM externe nâ€™est utilisÃ©.
- Si un client nâ€™a pas de documents ou si la requÃªte ne correspond Ã  rien, le systÃ¨me renvoie un message appropriÃ©.

------------------------------------------------------------
6. MÃ©thodologie
------------------------------------------------------------

1. **SÃ©paration des clients**  
   Chaque client est identifiÃ© par un en-tÃªte `X-API-KEY`. Le backend associe les clÃ©s aux clients et nâ€™accÃ¨de quâ€™aux documents du client. Cela garantit une sÃ©paration stricte entre locataires.

2. **RÃ©cupÃ©ration de documents**  
   Les documents de chaque client sont intÃ©grÃ©s Ã  lâ€™aide du modÃ¨le `SentenceTransformer` (`all-MiniLM-L6-v2`) et stockÃ©s dans un index FAISS.

3. **GÃ©nÃ©ration de rÃ©ponses**  
   Les requÃªtes sont intÃ©grÃ©es et le document le plus pertinent est rÃ©cupÃ©rÃ©.  
   > Les rÃ©ponses sont strictement limitÃ©es aux documents du client.

4. **Frontend**  
   Streamlit est utilisÃ© pour une interface simple. Les utilisateurs saisissent leur clÃ© API et leur question ; les rÃ©sultats sont renvoyÃ©s instantanÃ©ment.

5. **Pas de LLM externe**  
   Pour rester gratuit, aucune clÃ© OpenAI nâ€™est nÃ©cessaire. Le systÃ¨me fonctionne entiÃ¨rement hors ligne, produisant des rÃ©ponses strictement Ã  partir des documents.

------------------------------------------------------------
Structure des dossiers
------------------------------------------------------------

TestPython/
â”‚
â”œâ”€ app.py
â”œâ”€ ui.py
â”œâ”€ requirements.txt
â”œâ”€ README.txt
â””â”€ data/
   â”œâ”€ clientA/
   â”‚   â”œâ”€ docA1_procedure_resiliation.txt
   â”‚   â””â”€ docA2_produit_rc_pro_a.txt
   â””â”€ clientB/
       â”œâ”€ docB1_procedure_sinistre.txt
       â””â”€ docB2_produit_rc_pro_b.txt


ModÃ¨le utilisÃ© et raisons des mauvaises rÃ©ponses
1ï¸âƒ£ ModÃ¨le utilisÃ© dans le projet

Dans ce projet, nous avons initialement utilisÃ© le modÃ¨le :

EleutherAI / GPT-Neo 125M

CaractÃ©ristiques du modÃ¨le

ModÃ¨le open-source

Environ 125 millions de paramÃ¨tres

Taille â‰ˆ 500 MB

ModÃ¨le de gÃ©nÃ©ration de texte brute (language model)

Non instruction-tuned (pas entraÃ®nÃ© pour suivre des consignes)

ğŸ‘‰ Ce modÃ¨le est conÃ§u pour continuer un texte, pas pour rÃ©pondre correctement Ã  des questions.

2ï¸âƒ£ Comment fonctionne notre systÃ¨me RAG

Notre systÃ¨me est basÃ© sur une architecture RAG (Retrieval-Augmented Generation) :

Ã‰tape 1 â€“ Retrieval (recherche)

Les documents clients sont chargÃ©s depuis des fichiers .txt

Ils sont transformÃ©s en vecteurs via TF-IDF

La similaritÃ© entre la question et les documents est calculÃ©e avec cosine similarity

Le document le plus pertinent est sÃ©lectionnÃ©

âœ… Cette partie fonctionne correctement

Ã‰tape 2 â€“ Generation (gÃ©nÃ©ration de rÃ©ponse)

Le document trouvÃ© est injectÃ© dans un prompt

Le modÃ¨le GPT-Neo gÃ©nÃ¨re une rÃ©ponse Ã  partir de ce contexte

âŒ Câ€™est ici que le problÃ¨me apparaÃ®t

3ï¸âƒ£ Pourquoi le modÃ¨le rÃ©pond mal
ğŸ”´ 1. ModÃ¨le trop petit

GPT-Neo 125M est :

Trop limitÃ© en capacitÃ©

Incapable de comprendre des consignes complexes

Mauvais pour le raisonnement et lâ€™extraction dâ€™information

ğŸ”´ 2. Pas entraÃ®nÃ© pour rÃ©pondre Ã  des questions

Le modÃ¨le :

Nâ€™a pas Ã©tÃ© entraÃ®nÃ© avec des prompts du type
â€œRÃ©ponds Ã  la question Ã  partir du documentâ€

Ne sait pas quâ€™il doit extraire une information prÃ©cise

Peut produire des rÃ©ponses alÃ©atoires comme :

Â« Pourquoi ? Â»

des phrases incomplÃ¨tes

du texte sans rapport

ğŸ”´ 3. Hallucinations

Le modÃ¨le peut :

Inventer une rÃ©ponse

Ignorer le contenu rÃ©el du document

GÃ©nÃ©rer du texte qui semble cohÃ©rent mais est faux

Câ€™est un problÃ¨me classique avec les LLM non contrÃ´lÃ©s.

4ï¸âƒ£ Pourquoi ce problÃ¨me est critique dans un contexte SaaS / mÃ©tier

Dans un contexte professionnel (assurance, procÃ©dures, contrats) :

âŒ Une rÃ©ponse inventÃ©e est inacceptable

âŒ Une mauvaise information peut avoir un impact lÃ©gal

âŒ La fiabilitÃ© est plus importante que la crÃ©ativitÃ©

5ï¸âƒ£ Solution adoptÃ©e
âœ… Suppression de la gÃ©nÃ©ration LLM

Au lieu de demander au modÃ¨le de reformuler :

ğŸ‘‰ Le systÃ¨me renvoie directement le passage du document le plus pertinent

Avantages

âœ… RÃ©ponses 100% factuelles

âœ… BasÃ©es uniquement sur les documents clients

âœ… ZÃ©ro hallucination

âœ… Plus rapide et plus stable

âœ… AdaptÃ© Ã  un vrai usage SaaS